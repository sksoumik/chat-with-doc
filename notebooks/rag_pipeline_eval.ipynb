{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadman.soumik/code/chat-with-doc/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "def get_pdf_text(pdf_docs):\n",
    "    text = \"\"\n",
    "    for pdf in pdf_docs:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "def get_conversational_chain():\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.4)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
    "    return chain\n",
    "\n",
    "\n",
    "def get_vector_store(text_chunks):\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "\n",
    "def user_input(user_question):\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "    new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "    docs = new_db.similarity_search(user_question)\n",
    "    chain = get_conversational_chain()\n",
    "\n",
    "    response = chain(\n",
    "        {\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True\n",
    "    )\n",
    "\n",
    "    print(docs)\n",
    "\n",
    "    return response[\"output_text\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DOCS = [\n",
    "\"../input_data/2112_10668.pdf\",\n",
    "]\n",
    "\n",
    "text = get_pdf_text(PDF_DOCS)\n",
    "text_chunks = get_text_chunks(text)\n",
    "get_vector_store(text_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadman.soumik/code/chat-with-doc/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='considerably when the training set class distribution is uniform. These results highlight the severeness of\\nthemajority label bias issue in the multilingual in-context learning framework.\\nD.5 Knowledge Probing\\nWe evaluate to what extent our multilingual language model can effectively store factual knowledge in\\ndifferent languages. To this end, we evaluate knowledge triplet completion using the mLAMA dataset\\n(Kassner et al., 2021), which was translated from the English benchmark LAMA (Petroni et al., 2019)\\nusing Google Translate. The data is from TREx (Elsahar et al., 2018) with triples of the format ⟨object,\\nrelation, subject⟩. Following the convention of LAMA, triples are converted to templates for querying the\\nlanguage model. For example, a triple like ⟨Paris, capital-of, France ⟩is converted to template “Paris is the\\ncapital of [MASK] \". While each query in the original mLAMA dataset contains hundreds of candidates\\non average, we restrict it to three candidates one of which is the ground truth candidate and the other two\\ncandidates are randomly sampled to ensure fast inference and save API cost. Following the evaluation\\n23We implement our random sampling procedure using the numpy.random.choice function: https://numpy.org/\\ndoc/stable/reference/random/generated/numpy.random.choice.html .prec@1\\n0.40.50.60.70.8\\nenidptitcaesfidefrvietthtrbgkorueljaarzheuurbnhitaMultilingual, 7.5B GPT-3 CurieFigure A6: Knowledge probing on 25 languages. The performance of a random baseline is 0.33 since we down-\\nsampled the candidate set of each query to contain three candidates.\\nprotocol of mLAMA, we report precision @1 averaged over all relations per language.\\nWe evaluate on the 25 languages covered in XGLM’s pre-training data. We compare to the GPT-3 6.7B\\nmodel. As shown in Figure A6, both our multilingual model and GPT-3 Curie perform well on English.\\nFor non-English languages, our multilingual model maintains performance (above 0.6) while GPT-3\\nCurie drops drastically especially for medium and low resource languages. Overall, compared to an\\nEnglish-centric language model, our multilingual language model are better at retaining factual knowledge\\non a wider range of languages with +7.1points on average.\\nE Safety and Bias Analysis\\nGiven the centrality of large scale Language models, it is important to ensure such powerful models are\\nused responsibly. Accordingly, we further examine XGLM’s behavior on two tasks:\\n•Hate speech detection: A safety task to test language models’ ability to identify hateful and offensive\\ntext;\\n•Occupation Identification: A bias task to study language models’ performance disparity between\\ndifferent gender groups on the task of occupation identification.\\nThrough extensive experiments, we have following findings: First, hate speech detection in an in-context\\nlearning setting is quite challenging. Moreover, language models are not effectively leveraging few-\\nshot examples to improve the performance. Second, although language models have relatively good\\nperformance on the occupation identification task, they run the risk of exhibiting strong gender bias for\\ncertain occupations.\\nE.1 Hate Speech Detection\\nE.1.1 Setup\\nDatasets. We adopt datasets introduced by Huang et al. (2020) that include hate speech data from\\nTwitter in five languages: English, Italian, Portuguese, Polish and Spanish. All hyperlinks, usernames\\nand hashtags are replaced with generic symbols ( URL,USER ,HASHTAG ) to anonymize user information.\\nWe remove tweets containing more than 2generic symbols to encourage more informative examples.\\nWe further filter out tweets of length less than 5tokens or more than 30tokens. In the spirit of creating\\nbalanced data, we randomly sample 500each positive (hate speech) negative (not hate speech) examples\\nfor each language. For further comparison, we translate non-English data into English by using Google\\nTranslate and then evaluate English models performance on the task.\\nPrompts. We evaluate two approaches to prompting, similar to Section ??. For English prompts,\\nweprefix “The sentence is <candidate>” to the input sentence to form a prompt. We consider 10\\nverbalization candidates including 5negative ( normal., common., ok., usual., acceptable. ) corresponding\\nto classification of not hate speech and5positive ( sexist., racist., offensive., abusive., hateful. ) representing\\nclassification of hate speech . For code-switched prompt, we translate the English prefix and candidates\\ninto the corresponding target language by using Google Translate. For example, “The sentence is normal”Model language condition # shot English Italian Portuguese Polish Spanish\\nAccuracy\\nGPT-3 6.7Brepl. code switched0 54.5 54.3 57.0 51.3 52.5\\n4 55.5*53.4 48.5 52.5 52.2\\nGPT-3 6.7B code switched0 59.2 61.8 55.8 58.7 55.6\\n4 53.6 54.8 53.2 54.5 53.7\\nXGLM 7.5B code switched0 56.0 60.4 50.8 47.0 53.1\\n4 50.4 50.7 56.8 51.0 50.7\\nGPT-3 6.7Brepl. same language0 - 50.5 60.2 50.1 52.4\\n4 - 51.0 47.2 50.9 50.7\\nXGLM 7.5B same language0 - 57.5 41.8 50.0 53.1\\n4 - 53.2 56.5 51.1 52.7\\nGPT-3 6.7Brepl. English0 - 55.8 57.5 62.8*55.0\\n4 - 52.5 49.2 53.9 52.8\\nRecall\\nGPT-3 6.7Brepl. code switched0 57.0 90.2 67.0 21.6 77.0\\n4 73.0*76.7*72.5 65.4 77.1*\\nGPT-3 6.7B code switched0 62.4 88.6 51.5 49.0 76.4\\n4 65.7 69.2 59.6 58.5 61.1\\nXGLM 7.5B code switched0 77.2*95.4*80.4 53.8 95.8*\\n4 14.9 18.8 18.8 15.8 19.5\\nGPT-3 6.7Brepl. same language0 - 1.4 9.1 0.2 11.6\\n4 - 50.0 66.1 77.8 33.4\\nXGLM 7.5B same language0 - 87.4 39.5 0.0 79.4\\n4 - 39.0 24.9 55.8 44.6\\nGPT-3 6.7Brepl. English0 - 93.8 77.8 74.8*77.6\\n4 - 72.4 71.2 75.7 73.7\\nTable A8: Accuracy and recall scores of our multilingual model and other English models on the Hate Speech\\nDetection task. We evaluate five target languages. For each target language, we bold the highest number for zero-\\nshot and four-shot respectively. * indicates the number is significantly higher than others. For language condition,\\nwe consider three cases: “code switched” means the prefix, candidates are in English and tweets are in the target\\nlanguage; “same language” means prefix, candidates and tweets are in the target language; “English” means prefix,\\ncandidates and tweets are in English, i.e. note that, “same language\" and “English\" reduce to the same experimental\\ncondition when the target language is English.\\nis translated into “Questa frase è normale.” for Spanish. For few-shot learning, we randomly draw\\nexamples from the training data and report the average performance across 5runs.\\nMetrics. We compute precision, recall and accuracy for all experimental conditions. Since the test data\\nis balanced, the accuracy of a random baseline is 50%.\\nE.1.2 Results\\nWe show accuracy and recall scores in Table A8. Bolded results are the highest in the table and those with\\nan (*) are statistically significantly better than other comparable conditions. Hate speech detection is a\\nchallenging task for all models. We observe that across the five languages, in-context learning results are\\nonly slightly better than random ( 50%). The results are also unstable and sensitive to prompt changing.\\nOverall, the XGLM 7.5Bmodel has better recall compared to the English-centric models. For example, the\\nXGLM 6.7BEn-only model has very low recall score in the zero-shot setting with the language condition\\nset as “same language”, indicating that it blindly predicts almost everything as negative (not hate speech).\\nAnother interesting observation is that most few-shot results are worse than zero-shot, which indicates that\\nwith the prefix described above, language models are not able to utilize examples. Interestingly, we also\\nfind that in one-shot experiments models tend to copy the label of the given example instead of predicting\\nbased on the input tweet. This further demonstrates that language models are struggling with learning\\nfrom few-shot examples in this task.English Spanish French\\nModel Avg. ↑|Diff|↓Avg.↑|Diff|↓Avg.↑|Diff|↓\\nXGLM 6.7BEn-only 90.73 3.19 91.23 2.65 83.46 4.85\\nGPT-3 6.7B 90.42 3.53 86.91 5.18 90.85 2.30\\nXGLM 7.5B 86.49 2.83 82.93 4.28 76.55 2.95\\nXGLM 6.7BEn-only +translate - 3.19 91.18 3.75 90.07 2.35\\nTable A9: Accuracy and bias scores of our multilingual model and other English models on the occupation iden-\\ntification task. “|Diff|” stands for the average absolute accuracy gap between male and female groups aggregated\\nacross all occupations. We bold the highest accuracy score for each language.\\nE.2 Gender Bias in Occupation Identification\\nE.2.1 Setup\\nDatasets We use the English bio dataset introduced in (De-Arteaga et al., 2019) to study gender bias\\nbased on identifying a person’s occupation from their bios. For multilingual bio datasets we use those\\ncreated by (Zhao et al., 2020). Originally there are 28occupations in English, 69occupations in Spanish\\nand27occupations in French. To ensure we have plenty of test data for each occupation, we only keep\\noccupations with at least 1000 male examples and 1000 female examples. This leads to 16occupations in\\nEnglish, 6occupations in Spanish and 4occupations in French. We follow the setup in (Zhao et al., 2020)\\nwhere people’s names and pronouns are removed from the bios. We then prefix “The occupation of this\\nperson is <candidate>” to the input bio to form a prompt. The candidate set consists of five occupations,\\nincluding the ground truth one and four other randomly sampled male and female occupations (two male\\nand two female). Male (female) occupations refer to ones having predominantly more male (female)\\nsamples.\\nMetrics Similar to the metric for Hate Speech detection, we first obtain the scores for 5 candidates and\\nconsider a prediction correct if the ground truth candidate yields the highest score among five candidates.\\nWe then compute the bias score as the absolute gap between the accuracy scores on the male and female\\nsamples,24averaged across all occupations. A lower bias score indicates that a model has less divergence\\nin identifying occupations for men and women.\\nE.2.2 Results'), Document(page_content='training of a multilingual masked language model.\\nMultilingual pre-training. Early multilingual\\npre-training work train word embeddings over mul-\\ntilingual corpora (Mikolov et al., 2013). The\\nmultilingual versions of contextualized embed-\\nding models such as BERT (Devlin et al., 2019),\\nRoBERTa (Liu et al., 2019), BART (Lewis et al.,\\n2019) and T5 (Raffel et al., 2020) were also de-\\nveloped: mBERT (Devlin et al., 2019), XLM-R\\n(Conneau et al., 2020), mBART (Liu et al., 2020),\\nand mT5 (Xue et al., 2020). Such models were\\ntrained on a single, multilingual text corpus such as\\nmC4 (Xue et al., 2020) or CC25 (Liu et al., 2020).\\nSeveral approaches have been developed to fa-\\ncilitate cross-lingual transfer, including sub-word\\ntokenizers which enabled efficient, shared vocabu-\\nlary learning across languages (Kudo and Richard-\\nson, 2018), joint training for efficient knowledge\\ntransfer across languages (Pires et al., 2019; Jiang\\net al., 2020; Kassner et al., 2021), etc. A notable\\nconcurrent work is BLOOM16, which scales multi-\\nlingual pre-training to 46 languages and 175 billion\\nparameters.\\n6 Conclusion\\nWe introduce four multilingual generative language\\nmodels (XGLMs) at different scales, and study\\ntheir in-context few- and zero-shot learning capa-\\nbilities. We show that the few-shot learning capa-\\nbility of XGLM steadily improves as it scales. Our\\nlargest model (7.5B parameters) sets a new state\\nof the art for few-shot learning in more than 20\\nlanguages (including mid- and low-resource lan-\\nguages) on commonsense reasoning, NLI and ma-\\nchine translation tasks. An in-depth analysis shows\\nthe models are highly cross-lingual, which leads\\n16https://bigscience.huggingface.co/\\nblog/bloomto strong few-shot learning performance in non-\\nEnglish languages.\\nLimitations\\nAlthough the multilingual language model is an\\nimportant step towards building inclusive general-\\npurpose foundation models, our current models\\nhave the following limitations.\\nTraining Data. Our models are trained on a\\nstatic multilingual corpus extracted from Common-\\nCrawl, with English text comprising 32.6% of the\\ntotal number of tokens corresponding to 163B to-\\nkens. The English data portion of the corpus cor-\\nresponds to roughly 54% only of GPT-3’s training\\ndata. We applied several data filtering strategies as\\nproxies for data quality assurance (see a compre-\\nhensive list in the Data Card in Appendix F), such\\nas removing duplicated documents and paragraphs\\nby URLs, filtering out paragraphs with high ra-\\ntio of digits and punctuation, removing paragraphs\\nwith profanity, filtering by max number of URLs\\nand minimum length, etc. Such filtering may po-\\ntentially result in bias of the remaining data used\\nin pretraining, which would need further analy-\\nsis to understand. Furthermore, the raw data were\\ntaken from static CommonCrawl snapshots, which\\nmay not include entities and events beyond the time\\nspan of the snapshots (till March 2020), such as\\nCOVID-19, etc. As such we also note the potential\\ndifference in genres between CommonCrawl and\\nthe genres used in GPT-3 comprising in addition to\\nCommonCrawl, corpora such as BookCorpus and\\nWikipedia.\\nMoreover, GPT-3 is trained on 118 languages\\ndespite the fact that 93% of the data is English.17\\nIn contrast our models are trained on 30 languages\\nafter rigorous language identification and filtering.\\nPerformance on English tasks. As is shown in\\nSection 4.7 and Figure 2, our model underperforms\\nEnglish-centric models on eight tasks ranging from\\ncommonsense reasoning to QA. There are several\\nfactors which could be contributing to this gap,\\nsuch as\\n•Difference in training data quality (XGLM is\\ntrained on filtered CommonCrawl data only,\\nwhile the English-centric models are trained\\non data including both CommonCrawl as well\\n17https://github.com/openai/gpt-3/blob/\\nmaster/dataset_statistics/languages_by_\\nword_count.csvas high-quality corpora such as BookCorpus\\nand Wikipedia) and quantity (as is described\\nin the previous paragraph, the multilingual\\nmodel was trained on 54% of the English data\\nused in English-centric models);\\n•Curse of multilinguality. Previous work in\\nmultilingual training has shown that increas-\\ning the number of languages in model with\\nshared parameters hurts performance on all\\ntraining languages, e.g. English (Conneau\\net al., 2020).\\nAdditional experiments controlling for these factors\\nwould shed more light on the observed gap.\\nModel architecture and training objective. In\\nthis work, we only experimented with causal lan-\\nguage models with a decoder-only architecture,\\nwhich had previously demonstrated promising few-\\nshot learning capabilities (Brown et al., 2020).\\nHowever, such architecture and pretraining objec-\\ntive do not leverage bidirectional context such as\\nthose used by masked language models (MLM), or\\nsequence-to-sequence architectures with denoising\\nautoencoder pretraining objectives.\\nModel evaluation via in-context learning. We\\ncompare our language models to the baselines pri-\\nmarily in the in-context learning paradigm, using\\nthe same prompts for all language models in the\\ncomparison unless explicitly specified. Despite\\nminimal effort engineering the prompts for any\\nmodel, it is possible that the prompts work better\\nwith some models than the others, which intro-\\nduces bias to the evaluation. However, we expect\\nthis factor to have small impact and the relative\\nstrengths of the models can be reliably measured\\ngiven the volume of tasks they were evaluated on.\\nEvaluation on social value tasks for more lan-\\nguages. We evaluate and analyze the models’ per-\\nformance on hate speech detection and gender bias\\nfor professional occupations. These studies are\\nlimited by the available evaluation datasets. We\\nare limited in our study as we only investigate this\\nproblem space for six languages (English, French,\\nSpanish, Italian, Portuguese, and Polish) where a\\nmajority of them (5) pertain to the Romance lan-\\nguage family. It would be pertinent to investigate\\nthe impact of multilingual models on social value\\ntasks across a wider and more diversified set of\\nlanguages before drawing solid conclusions. More-\\nover, we contend that studies on other tasks suchas stereotype (Nangia et al., 2020; Nadeem et al.,\\n2021), ethics (Hendrycks et al., 2020) would pro-\\nvide a more comprehensive view of model behavior\\nfor social value tasks.\\nEthical Considerations\\nDevising multilingual pre-trained language models\\ncan serve as a powerful tool in the NLP arsenal for\\nmultiple reasons.\\nEnergy and maintenance efficiency. From an\\nengineering perspective, XGLM pertains to a fam-\\nily of models that represent single unified models\\ncatering to many languages which have wide ap-\\nplication across many applications. Such a unified\\nsingle model saves on carbon footprint as well as\\nenergy consumption (comparing to the alternative:\\nseparate models for different languages) leading to\\nmore energy efficiency. A single model, despite\\nhaving the risk of being a single point of failure, has\\nthe powerful incentive of being easier to maintain,\\naccess, distribute, and track.\\nDiversity and inclusion. Models such as XGLM\\nrepresent a paradigm shift from the Anglo-centric\\nview of the world of NLP to being able to cater to\\nall languages on an equal footing. Paying attention\\nto the design of such models is critical to ensure\\nequitability and inclusion, exemplified here by at-\\ntempting to balance language representation. The\\nfurther power of XGLM specifically is its ability\\nto perform comparably to Anglo-centric models\\nin zero to few shot settings. Possessing powerful\\nmultilingual models that can perform well in such\\nsettings especially for medium to extremely low re-\\nsource languages helps alleviate the burden of cre-\\nating supervised data for such languages especially\\nfor economically challenged languages (medium to\\nlow digital presence typically goes hand in hand\\nwith economic disparities). Moreover, having such\\nmodels catering to scarcer languages spurs scien-\\ntific research in such languages leading to more\\ndiversified NLP, and more diversified science in\\nthe broader sense.\\nSocial values. We further investigate the impact\\nof our models on social valued problems such as\\nhate speech detection and bias (Appendix §E). De-\\nspite inconclusive results overall (bordering on neg-\\native), we note that for the relatively scarcer data\\nsetting (Polish) the multilingual models outperform\\nthe Anglo-centric models indicating that XGLM\\nwill be performant for less resourced languages.This is especially significant for social value tasks\\nwhere obtaining training data is quite problematic\\ndue to the inherent expense of obtaining high qual-\\nity annotated data.\\nTransparency and Accountability. In the spirit\\nof transparency and accountability for large-scale\\nlanguage modeling we include detailed model card\\nand data card with the model and paper release.\\nReferences\\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2020.\\nTranslation artifacts in cross-lingual transfer learn-\\ning. In Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Process-\\ning, EMNLP 2020, Online, November 16-20, 2020 ,\\npages 7674–7684. Association for Computational\\nLinguistics.\\nYonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng\\nGao, and Yejin Choi. 2020. PIQA: reasoning about\\nphysical commonsense in natural language. In The\\nThirty-Fourth AAAI Conference on Artificial Intel-\\nligence, AAAI 2020, The Thirty-Second Innovative\\nApplications of Artificial Intelligence Conference,\\nIAAI 2020, The Tenth AAAI Symposium on Edu-\\ncational Advances in Artificial Intelligence, EAAI\\n2020, New York, NY, USA, February 7-12, 2020 ,\\npages 7432–7439. AAAI Press.\\nTerra Blevins and Luke Zettlemoyer. 2022. Lan-\\nguage contamination explains the cross-lingual ca-\\npabilities of english pretrained models. CoRR ,\\nabs/2204.08110.\\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\\nAltman, Simran Arora, Sydney von Arx, Michael S\\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\\nBrunskill, et al. 2021. On the opportunities'), Document(page_content='ceedings of the 57th Conference of the Association\\nfor Computational Linguistics, ACL 2019, Florence,\\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\\npers, pages 4791–4800. Association for Computa-\\ntional Linguistics.\\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng,\\nZhen Bi, Chuanqi Tan, Fei Huang, and Huajun\\nChen. 2021. Differentiable prompt makes pre-\\ntrained language models better few-shot learners.\\nCoRR , abs/2108.13161.\\nJieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini,\\nKai-Wei Chang, and Ahmed Hassan Awadallah.\\n2020. Gender bias in multilingual embeddings and\\ncross-lingual transfer. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computa-\\ntional Linguistics , pages 2896–2907.\\nMengjie Zhao and Hinrich Sch ¨utze. 2021. Discrete\\nand soft prompting for multilingual models. CoRR ,\\nabs/2109.03630.\\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\\nSameer Singh. 2021. Calibrate before use: Im-\\nproving few-shot performance of language models.\\nInProceedings of the 38th International Confer-\\nence on Machine Learning, ICML 2021, 18-24 July\\n2021, Virtual Event , volume 139 of Proceedings of\\nMachine Learning Research , pages 12697–12706.\\nPMLR.\\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2019. Aligning books and movies:\\nTowards story-like visual explanations by watch-\\ning movies and reading books. arXiv preprint\\narXiv:1506.06724 .A Pretraining Details\\nXGLM. All models are trained with the Fairseq library (Ott et al., 2019). We use Adam optimizer with\\n𝛽1= 0.9,𝛽2= 0.98,𝜖= 1𝑒−8. We adjust the learning rate based on model size, e.g. 1.5𝑒−3for the\\n564M and 1.7B model, 7.5𝑒−4for the 2.9B model, and 1.2𝑒−4for the 7.5B models. Learning rates\\nwere adjusted with a 2000 warm-up updates followed by a polynomial decay schedule. All models are\\ntrained with data parallel and an effective batch size of 4M tokens. The XGLM 7.5B model was trained\\non 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second18.\\nGPT-3 6.7Brepl..We replicate the GPT-3 6.7Barchitecture and optimization hyperparameters to the best of\\nour knowledge for training this model. The most significant difference between this model and GPT-3 6.7B\\nis in the training data. The training data used by GPT-3 6.7Brepl. is a combination of six English-language\\ndatasets, totaling 453GB and 112B tokens (which we up-sampled to 300B tokens):\\n•BookCorpus (Zhu et al., 2019), a dataset consisting of more than 10K unpublished books (4GB);\\n•English Wikipedia , excluding lists, tables and headers (12GB);\\n•CC-News (Nagel, 2016), a dataset containing 63 millions English news articles crawled between\\nSeptember 2016 and February 2019 (76GB);\\n•OpenWebText (Gokaslan and Cohen, 2019), an open source recreation of the WebText dataset used to\\ntrain GPT-2 (38GB);\\n•CC-Stories (Trinh and Le, 2018), a dataset containing a subset of CommonCrawl data filtered to match\\nthe story-like style of Winograd schemas (31GB);\\n•English CC100 (Wenzek et al., 2020), a dataset extracted from CommonCrawl snapshots between\\nJanuary 2018 and December 2018, filtered to match the style of Wikipedia (292GB).\\nThe data are encoded using the same Byte-Pair Encoding (BPE) as GPT-2 (Radford et al., 2019) and\\nRoBERTa (Liu et al., 2019) with a vocabulary of 50K subword units.\\nA.1 Validation Perplexity\\nWe use in-domain validation perplexity to validate the convergence status of the models. Figure A1 shows\\nthe average perplexity of the four models evaluated using a validation dataset sampled from CC100-XL.\\nThe validation data contains 30k sentences for each language that do not overlap with the pre-training\\ndata. We group the results by resource level.\\n1.02.03.04.05.06.07.0\\nmodel params. (B)8.010.012.014.016.018.020.0valid ppl\\nhi\\nmed\\nlo\\nex-lo\\nFigure A1: XGLM perplexity on CC100_XL validation set as a function of model size.\\n18On 256 A100 GPUs, the inference speed can reach 1.47 million words per second. Besides, inference can be done with\\nsignificantly less resources. For example, using 8 v100 GPUs, it took 6 hrs to evaluate XGLM 7.5B on XStoryCloze.B Multilingual In-context Learning Formulation\\nWe extend the in-context learning framework proposed by Brown et al. (2020) to the multilingual setting.\\nLetℳbe a causal language model and 𝒟be a task.𝒟= (𝒫,ℰ)consists of a task description 𝒫and a\\nfew demonstration examples in one or more languages\\nℰ=|ℒ|⋃︁\\n𝑙=1ℰ𝑙.\\nWe consider the setting where the task description comes in the form of a prompt 𝒫= (𝒯, 𝑣).𝒯is a\\ncloze-style template that converts an example input 𝑥into a string𝒯(𝑥)that contains a [Mask] symbol.19\\nFor classification and multiple-choice problems, 𝑣:𝒴→𝒱*is a verbalizer that maps each candidate\\nlabel or choice 𝑦∈𝒴into a string 𝑣(𝑦). Both𝒯(𝑥)and𝑣(𝑦)can be tokenized into a sequence of one\\nor more tokens in the language model vocabulary 𝒱. An instantiated prompt 𝒫(𝑥, 𝑦)is obtained by\\nsubstituting the [Mask] symbol in𝒯(𝑥)with𝑣(𝑦). Table 2 shows the prompts used by all tasks in our\\nmain experiments.\\nZero-shot learning. Given a test example ˜𝑥𝑡in any target language 𝑡, the zero-shot prediction is ^𝑦\\nwhich maximizes a language model based scoring function ( §C.2).\\n^𝑦= arg max\\n𝑦𝜎(ℳ,𝒫(˜𝑥𝑡, 𝑦)). (1)\\nThis general formulation can cover most NLP tasks. For classification problems, 𝑣is a mapping from\\nclasses to strings; for multiple-choice problems, 𝑣is an identity function that maps each candidate choice\\nto itself. For text generation problems, 𝑣is identity and we decode free-form text from [Mask] , which in\\nthis case is positioned at the end of 𝒯(𝑥).\\nFew-shot learning. Suppose we have 𝑘demonstration examples available in a source language:\\nℰ𝑠={(𝑥𝑠\\n𝑖, 𝑦𝑖)}𝑘\\n𝑖=1.\\nIn this case, we concatenate the instantiated prompts of the demonstration examples {𝒫(𝑥𝑠\\n𝑖, 𝑦𝑖)}𝑘\\n𝑖=1and\\nmake it the prefix of the input string used in the zero-shot learning setting to form the objective:\\n^𝑦= arg max\\n𝑦𝜎(𝒫(𝑥𝑠\\n1, 𝑦1)[Sep] . . .𝒫(𝑥𝑠\\n𝑘, 𝑦𝑘)[Sep]𝒫(˜𝑥𝑡, 𝑦)), (2)\\nwhere [Sep] is a separator symbol chosen empirically.\\n•When 𝑠=𝑡, we have the in-language few-shot learning setup.\\n•When 𝑠̸=𝑡, we have the cross-lingual few-shot learning setup.\\nC Evaluation Details\\nC.1 English Evaluation Tasks\\nTable A1 shows all the English tasks used in our evaluation.\\n19We relaxed the prompt format of GPT-3 by allowing the [Mask] symbol to appear anywhere in 𝒯(𝑥)instead of just in\\nthe end. Having this additional flexibility leads to better performance on some tasks. This is inspired by the masked language\\nmodeling prompts constructed by recent work (Schick and Sch ¨utze, 2021; Zhang et al., 2021).ReasoningStoryCloze (Mostafazadeh et al., 2016) – 1,871 1,871 N/A 1\\nCOPA‡(Gordon et al., 2012) 400 100 499 N/A 1\\nWinoGrande (Sakaguchi et al., 2020) 40,398 1,267 1,767 N/A 1\\nHellaSwag (Zellers et al., 2019) 39,905 10,042 10,003 N/A 1\\nQAARC-easy (Clark et al., 2018) 2,251 570 2,376 N/A 1\\nARC-cha. (Clark et al., 2018) 1,119 299 1,172 N/A 1\\nPIQA (Bisk et al., 2020) 16,113 1,838 3,084 N/A 1\\nOpenbookQA (Mihaylov et al., 2018) 4,957 500 500 N/A 1\\nTable A1: English tasks used in our few-shot learning evaluation. All tasks use accuracy as the evaluation metrics.\\nC.2 Scoring Functions\\nWe considered the following functions for scoring an instantiated prompt using a language model:\\n(1) sum of per-token log probabilities;\\n(2) average of per-token log probabilities;\\n(3) average of per-token log probabilities, ignoring the common prefix of different candidates.\\nWe also considered the calibration approach proposed by Zhao et al. (2021) and character normalization\\nproposed by Lieber et al. (2021).\\nIn the end, we use the average of per-token log-probabilities ignoring the common prefix of different\\ncandidates as the scoring function for all multilingual tasks. This is selected based on the development set\\nperformance of StoryCloze and XNLI.\\nFor English tasks, we use the same modeling choices as Brown et al. (2020). Specifically, we use the\\ntask prompts as detailed in Appendix G of Brown et al. (2020), and a single newline as the separator for\\nfew-shot learning. For WinoGrande, we take the log-likelihood of the common suffix of the different\\ncandidates as the scoring function. For ARC-easy, ARC-challenge and OpenBookQA, we normalize by the\\nunconditional probability of each candidate by taking𝑝(completion|context )\\n𝑝(completion|answer _context ), where we use the string\\n“Answer: ” as answer_context. For all the other tasks, we take the average of per-token log-probabilities,\\nignoring the common prefix of the different candidates.\\nC.3 Evaluation Protocol\\nAll few-shot learning results are obtained with the in-language setting (both the training and test examples\\nare in the same language) unless otherwise specified. We report results on the test set for all multilingual\\ntasks (including the held-out tasks). For English tasks, we report results on the test set for ARC-easy,\\nARC-challenge, OpenBookQA and StoryCloze, and on the development set for the rest, following Brown\\net al. (2020). For few-shot learning, we report the average results across 5 runs, randomly sampling\\na different set of few-shot examples each time. For tasks with a training set, we sample the few-shot\\nexamples from the training set; for tasks with no training set, we sample from the dev set and report\\nevaluation results on the test set; for dev-set examples on XNLI and XCOPA, we sample few-shot examples\\nfrom the test set, since these two tasks do not have the training sets for all languages. While Brown et al.\\n(2020) tuned the few-shot value 𝑘as a hyperparameter on the dev set, we pre-selected a few 𝑘values (0,\\n1, 4, 32, 128) and report the corresponding results.\\nC.3.1 Example Truncation\\nFollowing Brown et al. (2020), we truncate the input such that they fit the maximum context length of\\nXGLM ( 𝑛ctx= 2048 ) and preserve only the complete demonstration examples after truncation. For each\\ntask, we report results up to the 𝑘’s corresponding to the maximum fit.20Table A2 shows the average'), Document(page_content='English, 6occupations in Spanish and 4occupations in French. We follow the setup in (Zhao et al., 2020)\\nwhere people’s names and pronouns are removed from the bios. We then prefix “The occupation of this\\nperson is <candidate>” to the input bio to form a prompt. The candidate set consists of five occupations,\\nincluding the ground truth one and four other randomly sampled male and female occupations (two male\\nand two female). Male (female) occupations refer to ones having predominantly more male (female)\\nsamples.\\nMetrics Similar to the metric for Hate Speech detection, we first obtain the scores for 5 candidates and\\nconsider a prediction correct if the ground truth candidate yields the highest score among five candidates.\\nWe then compute the bias score as the absolute gap between the accuracy scores on the male and female\\nsamples,24averaged across all occupations. A lower bias score indicates that a model has less divergence\\nin identifying occupations for men and women.\\nE.2.2 Results\\nWe present the overall accuracy scores and the bias scores (|Diff|) in Table A9. Results indicate that the\\nXGLM 6.7BEn-only model achieves the best performance on English and Spanish, while the GPT-3 6.7B\\nmodel achieves the best performance on French. XGLM 7.5Bmodel, instead, falls behind on all three\\nlanguages, especially for Spanish and French. We think this is potentially due to that all pronouns and\\npeople’s names are removed from the test data but not training data. The training data for XGLM 7.5B\\ncontains more Spanish and French compared to the other two models. Thus, XGLM 7.5Bmay have more\\nsevere morphological mismatch on Spanish and English. Regarding the bias score, the GPT-3 6.7Bmodel\\nis the most biased model on both English and Spanish but least biased on French. XGLM 6.7BEn-only\\nand XGLM 7.5Bexhibit the least bias on Spanish and English, respectively.\\nF Data Card\\nWe follow the recommendations of Gebru et al. (2018) and provide a datacard for the dataset used to train\\nXGLM, which is a subset of CC100-XL, a larger multilingual dataset we curated.\\nF.1 Data Sources\\nFollowing the recent success of multilingual self-supervised pre-training (Devlin et al., 2019; Lample and\\nConneau, 2019; Conneau et al., 2020; Xue et al., 2020; Goyal et al., 2021a; Liu et al., 2020), we train\\nour language models on a mixture of monolingual text of different languages. We extend the pipeline\\nused for mining the CC100 corpus (Conneau et al., 2020; Wenzek et al., 2020) to generate CC100-XL,\\n24We only consider gaps that are statistically significant.a significantly larger multilingual dataset covering 68 Common Crawl (CC) snapshots (from Summer\\n2013 to March/April 2020) and 134 languages. As the first step to balance the language distribution, we\\nsampled 30% of the data from the languages that contain more than 15 billion tokens and more than 20\\nmillion documents. This resulted in a 8.4 TB multilingual corpus with 1.9 trillion tokens.\\nF.2 Motivation\\n•For what purpose was the dataset created? Was there a specific task in mind? Was there a\\nspecific gap that needed to be filled? Please provide a description. The CC100-XL dataset was\\ncollected to create a high quality monolingual dataset for at least 100 languages. It was mainly\\nused for training foundation multilingual language models which may be applied to a broad list of\\nlanguage tasks, including neural machine translation, speech translation, question answering, etc.\\nCC100-XL involves sentence level filtering, preserves context, improves the filtering mechanism,\\nand paves a way for mining 200+ languages.\\n•Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,\\ncompany, institution, organization)? Meta AI.\\n•Who funded the creation of the dataset? If there is an associated grant, please provide the\\nname of the grantor and the grant name and number. Meta AI.\\n•Any other comments? No.\\nF.3 Composition\\n•What do the instances that comprise the dataset represent (e.g., documents, photos, people,\\ncountries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and\\ninteractions between them; nodes and edges)? Please provide a description. The instances are\\ntextual documents sampled from Commoncrawl snapshots.\\n•How many instances are there in total (of each type, if appropriate)? The training dataset of\\nXGLM contains 1.74 billion documents in total.\\n•Does the dataset contain all possible instances or is it a sample (not necessarily random) of\\ninstances from a larger set? If the dataset is a sample, then what is the larger set? Is the\\nsample representative of the larger set (e.g., geographic coverage)? If so, please describe how\\nthis representativeness was validated/verified. If it is not representative of the larger set, please\\ndescribe why not (e.g., to cover a more diverse range of instances, because instances were\\nwithheld or unavailable). The dataset is a subset of CC100-XL. For each language, the data is\\neither a full set or a random subset of CC100-XL data. Especially, the medium- and low-resource\\nlanguages are upsampled. In terms of language representation, the CC100-XL dataset contains 134\\nlanguages extracted using fasttext25from Common Crawl snapshots. We further selected a subset\\nof 30 languages to train XGLM, taking geo-location, language family and typology diversity of the\\nlanguages into account.\\n•What data does each instance consist of? “Raw” data (e.g., unprocessed text or images) or\\nfeatures? In either case, please provide a description. Each instance consists of raw text data.\\n•Is there a label or target associated with each instance? If so, please provide a description. No.\\n•Is any information missing from individual instances? If so, please provide a description,\\nexplaining why this information is missing (e.g., because it was unavailable). This does not\\ninclude intentionally removed information, but might include, e.g., redacted text. No.\\n25https://fasttext.cc/docs/en/language-identification.htmlISO code Language Tokens (M) Size (GiB) ISO code Language Tokens (M) Size (GiB)\\nen English 803,527 3,324.45 - Arabic Romanized 685 1.65\\nru Russian 147,792 1,007.38 mn Mongolian 681 4.26\\nzh Chinese 132,770 485.32 la Latin 635 2.20\\nde German 89,224 369.30 ne Nepali 600 5.32\\nes Spanish 87,303 363.83 si Sinhalese 524 3.96\\nfr French 77,420 303.76 mr Marathi 458 3.59\\nja Japanese 66,054 293.39 kn Kannada 446 3.41\\nit Italian 41,930 170.76 so Somali 436 1.56\\npt Portuguese 36,586 147.12 cy Welsh 398 1.27\\nel Greek 28,762 180.37 jv Javanese 389 1.23\\nro Romanian 24,176 93.63 ps Pashto 387 1.97\\nuk Ukrainian 23,723 156.68 uz Uzbek 332 1.64\\nhu Hungarian 22,718 89.87 gu Gujarati 327 2.10\\nko Korean 20,002 79.08 km Khmer 272 2.14\\npl Polish 19,293 73.59 - Urdu Romanized 245 0.73\\nno Norwegian 17,600 70.89 am Amharic 169 0.85\\nnl Dutch 17,163 68.36 - Bengali Romanized 166 0.48\\nfi Finnish 16,804 67.28 pa Punjabi 153 0.93\\nda Danish 16,274 64.74 gl Galician 137 0.50\\nid Indonesian 15,424 67.51 ha Hausa 124 0.42\\nhr Croatian 14,455 54.27 mg Malagasy 116 0.38\\ntr Turkish 12,413 51.51 sa Sanskrit 107 0.42\\nar Arabic 12,249 64.34 eu Basque 105 0.35\\nvi Vietnamese 11,199 50.45 my Burmese 101 0.74\\nth Thai 10,842 99.86 su Sundanese 91 0.30\\nbg Bulgarian 9,704 61.10 or Oriya 91 0.62\\nfa Persian 9,355 57.46 ht Haitian 87 0.28\\nsv Swedish 9,169 36.54 lo Lao 84 0.59\\nms Malay 9,106 38.57 ky Kyrgyz 70 0.34\\nhe Hebrew 8,637 42.13 br Breton 57 0.16\\ncs Czech 8,616 32.46 ga Irish 49 0.15\\nsk Slovak 8,251 30.70 yo Yoruba 48 0.14\\nca Catalan 7,076 26.90 eo Esperanto 47 0.14\\nlt Lithuanian 4,847 18.38 - Tamil Romanized 40 0.13\\nsl Slovene 4,029 14.97 zu Zulu 40 0.14\\nhi Hindi 3,448 26.63 ti Tigrinya 40 0.19\\net Estonian 3,287 12.18 - Telugu Romanized 37 0.11\\nlv Latvian 2,815 10.67 ku Kurdish 36 0.10\\ntl Tagalog 2,389 8.13 om Oromo 27 0.09\\nsq Albanian 2,382 8.76 xh Xhosa 26 0.09\\nsr Serbian 2,101 12.68 gd Scottish Gaelic 19 0.05\\n- Hindi Romanized 2,045 6.60 ig Igbo 18 0.06\\naz Azerbaijani 1,904 8.41 as Assamese 17 0.10\\nbn Bengali 1,627 11.19 lg Ganda 15 0.05\\nta Tamil 1,477 12.36 wo Wolof 14 0.03\\nur Urdu 1,352 7.77 fy Western Frisian 12 0.04\\nkk Kazakh 1,278 8.40 tn Tswana 11 0.03\\nhy Armenian 1,261 7.16 ff Fula 11 0.03\\nka Georgian 1,261 10.48 gn Guaraní 10 0.03\\nis Icelandic 1,163 4.21 sd Sindhi 8 0.04\\nbe Belarusian 1,004 5.81 ln Lingala 7 0.02\\nbs Bosnian 950 4.00 bm Bambara 6 0.02\\nml Malayalam 935 8.08 iu Inuktitut 6 0.03\\nmk Macedonian 927 6.05 kg Kongo 4 0.01\\nsw Swahili 908 3.19 qu Quechua 3 0.01\\naf Afrikaans 819 3.04 ss Swati 2 0.01\\nte Telugu 689 5.28 - Unassigned 503 2.30\\nTable A10: Languages and statistics of the training data set selected from CC100 XL corpus.•Are relationships between individual instances made explicit (e.g., users’ movie ratings, social\\nnetwork links)? If so, please describe how these relationships are made explicit. A small\\npercentage of document instances (<2%) are duplicated. Other than that, there are no relationships\\nbetween individual instances.\\n•Are there recommended data splits (e.g., training, development/validation, testing)? If so,\\nplease provide a description of these splits, explaining the rationale behind them. This dataset\\nis split into training and validation only. For each high resource language, at least 5,000 randomly\\nselected documents and 30,000 lines were split into validation set, and the rest documents are for\\ntraining; for low-resource languages, at least 100 randomly selected documents and 1,000 lines (a\\ncouple of very low resource languages contain 80 documents) were split into valid set and leave the\\nrest for training. There are 3.5 million lines of text in total for the validation set. This split is mainly\\nto ensure a good size of validation data with the coverage and balance over all languages, meanwhile,\\nthe validation size is not too large to affect the overall training speed.\\n•Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hate speech detection and Occupation Identification'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input(\"What are the two tasks used to examine XGLM's behavior in the context of responsible use of large scale language models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = get_text_chunks(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What are the two tasks used to examine XGLM's behavior in the context of responsible use of large scale language models?\",\n",
    "    \"What is the purpose of the occupation identification task in the study of XGLM's behavior?\",\n",
    "    \"What is the scope of the multilingual dataset CC100-XL used for training language models in terms of time coverage and language diversity?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"The two tasks used to examine XGLM's behavior are hate speech detection, which tests the model's ability to identify hateful and offensive text, and occupation identification, which studies the model's performance disparity between different gender groups in identifying occupations.\",\n",
    "    \"The purpose of the occupation identification task is to study gender bias in language models by analyzing their performance disparity between different gender groups on the task of identifying a person's occupation from their bios.\",\n",
    "    \"The CC100-XL dataset covers 68 Common Crawl snapshots from Summer 2013 to March/April 2020 and includes 134 languages. It is a significantly larger multilingual dataset with a corpus of 8.4 TB and 1.9 trillion tokens, designed to balance language distribution by sampling data from languages with more than 15 billion tokens and 20 million documents.\"\n",
    "]\n",
    "\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Inference\n",
    "\n",
    "for query in questions:\n",
    "    answer = user_input(query)\n",
    "    answers.append(answer)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
