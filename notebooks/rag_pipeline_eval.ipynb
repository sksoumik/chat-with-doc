{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadman.soumik/code/chat-with-doc/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "def get_pdf_text(pdf_docs):\n",
    "    text = \"\"\n",
    "    for pdf in pdf_docs:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "def get_conversational_chain():\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.4)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
    "    return chain\n",
    "\n",
    "\n",
    "def get_vector_store(text_chunks):\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "\n",
    "def user_input(user_question):\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "    new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "    docs = new_db.similarity_search(user_question)\n",
    "    chain = get_conversational_chain()\n",
    "\n",
    "    response = chain(\n",
    "        {\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True\n",
    "    )\n",
    "\n",
    "    return response[\"output_text\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DOCS = [\n",
    "\"../input_data/2112_10668.pdf\",\n",
    "]\n",
    "\n",
    "text = get_pdf_text(PDF_DOCS)\n",
    "text_chunks = get_text_chunks(text)\n",
    "get_vector_store(text_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = get_text_chunks(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What are the two tasks used to examine XGLM's behavior in the context of responsible use of large scale language models?\",\n",
    "    \"What is the purpose of the occupation identification task in the study of XGLM's behavior?\",\n",
    "    \"What is the scope of the multilingual dataset CC100-XL used for training language models in terms of time coverage and language diversity?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"The two tasks used to examine XGLM's behavior are hate speech detection, which tests the model's ability to identify hateful and offensive text, and occupation identification, which studies the model's performance disparity between different gender groups in identifying occupations.\",\n",
    "    \"The purpose of the occupation identification task is to study gender bias in language models by analyzing their performance disparity between different gender groups on the task of identifying a person's occupation from their bios.\",\n",
    "    \"The CC100-XL dataset covers 68 Common Crawl snapshots from Summer 2013 to March/April 2020 and includes 134 languages. It is a significantly larger multilingual dataset with a corpus of 8.4 TB and 1.9 trillion tokens, designed to balance language distribution by sampling data from languages with more than 15 billion tokens and 20 million documents.\"\n",
    "]\n",
    "\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Inference\n",
    "\n",
    "for query in questions:\n",
    "    answer = user_input(query)\n",
    "    answers.append(answer)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
